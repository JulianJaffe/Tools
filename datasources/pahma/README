The code in this directory extracts data from PAHMA CSpace into csv files.

It was originally intended to provision the Delphi system, and the query exracts fields used by that system.

However, it is currently used to provision the Solr4 datasources used by PAHMA (and other UCB deployments)

To run the ad hoc ETL  used in the pahma solr datasource, do something like the following in this directory:

$ nohup ./makeCsv4.sh pahma &

or, via crontab, something like the following:

[jblowe@pahma-dev pahma]$ crontab -l
0 3 * * * /home/developers/pahma/makeCsv4.sh pahma >> /home/developers/pahma/extract.log.txt  2>&1

The script does the following:

* Extracts via sql the metadata needed for each object
* It does this incrementally via a set of sql query which are stitched together by join.py
* The various parts are merged into a single metadata file containing multi-valued fields, latlongs, etc.
* Extracts via sql the media (blob) metadata needed for each object
* Merges the two (i.e. adds the blob csid as a multivalued field to the metadata file)
* Clears out the pahma-metadata solr4 core
* Loads the merged .csv file into solr.

The script currently take about 20 minutes to run.

Caveats:

- the query, its results, and the resulting solr datasource are largely unverified. Caveat utilizator.

(jbl 6/15/2014)
